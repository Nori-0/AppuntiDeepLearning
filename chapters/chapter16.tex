\chapter{GNN \& GCN}

Molti dati del mondo reale non seguono delle strutture regolari, proprio in questi casi, i grafi offrono una rappresentazione più adatta: strutture composte da nodi (gli elementi) e archi (le connessioni tra essi), in grado di modellare relazioni complesse come quelle presenti nei social network, nelle molecole, nei testi, nelle immagini, nei sistemi biologici e in quelli di raccomandazione. Questo capitolo è dedicato al Graph Machine Learning, ovvero all'applicazione del Deep Learning ai dati rappresentati come grafi. Analizzeremo le Graph Neural Networks (GNN) e le principali famiglie di modelli, i problemi che possono risolvere e le tecniche per rappresentare e manipolare i dati strutturati all’interno di una rete neurale, e ci soffermeremo sulle Graph Convolutional Networks (GCN). Approfondiremo poi modelli più recenti e avanzati, come le Graph Attention Networks (GAT), GraphSAGE e le Graph Isomorphism Networks (GIN). 
\begin{figure}[hb]
    \centering
    \includegraphics[width=0.55\textwidth]{figure/Graph.png}
    \caption{Esempio di grafo non orientato composto da 6 nodi e 5 archi. Se fosse orientato, gli archi avrebbero delle frecce che ne indicherebbero la direzione.}
    \label{fig:enter-label}
\end{figure}
\section{Grafi}

Un \textbf{grafo} ($G=(V,E)$) è una struttura flessibile in grado di rappresentare informazioni interconnesse. È composto da elementi chiamati \textit{vertici} ($V$ o nodi), connessi tra loro mediante \textit{Archi} ($E$ o \textit{Spigoli}), a differenza di una lista o di una sequenza, un grafo non presenta un \textbf{ordine implicito}, ed è quindi ideale per descrivere dati complessi che non seguono un ordine lineare. 

\begin{itemize}
    \item Gli archi possono essere unidirezionali o bidirezionali, definendo i grafi orientati o non orientati;
    \item Ogni componente possiede degli attributi: i nodi sono caratterizzati da feature (vettori di attributi), dalla loro identità o dal numero di connessioni (grado); gli archi possono avere un peso che quantifica la forza della connessione;
    \item Talvolta, si introduce anche un vettore di Contesto Globale ($\mathcal{U}$ o Master Node), che sintetizza le informazioni dell'intero grafo.
\end{itemize}

\subsection{Immagini come grafi}

Le immagini possono essere descritte tramite grafi a struttura regolare (griglie). Si effettua una corrispondenza fra pixel e nodo, connettendoli fra loro in base all'adiacenza spaziale (spesso 4 o 8 vicini). L'informazione contenuta in ciascun nodo è il vettore RGB (o altri canali) del pixel. Per rappresentare le connessioni tra i nodi in modo computabile si utilizza la \textbf{Matrice di Adiacenza} ($A$, Figura~\ref{fig:adjMatrix}) , una struttura che indica quali nodi sono connessi, facilitando le operazioni computazionali sul grafo.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figure/AdjacencyMatrix}
    \caption{Matrice di adiacenza per un'immagine $5\times5$ raffigurante una faccina sorridente. I nodi (pixel) sono connessi se condividono un lato.}
    \label{fig:adjMatrix}
\end{figure}

\subsection{Testi come grafi}
Modellare testi come grafi non è la pratica più comune in \textbf{NLP} (Natural Language Processing) perché questi dati possiedono una struttura regolare e lineare (sequenza di token). Nei testi, ogni parola è canonicamente connessa solo a quella precedente e a quella successiva, producendo matrici di adiacenza fortemente strutturate, spesso con una diagonale che riflette questa linearità. L’utilizzo dei grafi in NLP diventa vantaggioso solo quando si vogliono modellare relazioni complesse e irregolari che esulano dalla semplice sequenza (e.g\textit{ Semantic Parsing, Dependency Trees, o Knowledge Graphs}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/TextGraph.png}
    \caption{Matrice di adiacenza di un testo: ogni parola è connessa alla precedente e alla successiva, dando origine a una struttura diagonale.}
    \label{fig:textGraph}
\end{figure}

\subsection{Task sui grafici}

Le GNN possono essere applicate su tre diverse tipologie di task:

\begin{itemize}
    \item \textbf{Graph-level task:} Obiettivo di predire una proprietà che riguarda l’intero grafo (un singolo output);
    \begin{itemize}
        \item \textit{Esempio:} Prevedere la tossicità, l'odore, o la funzione di una molecola (modellata come grafo);
        \item \textit{Parallelo:} Classificazione di immagini; Sentiment Analysis di un intero documento;
    \end{itemize}
    \item \textbf{Node-level task:} Obiettivo di classificare o predire una proprietà per ciascun nodo individualmente;
    \begin{itemize}
        \item \textit{Esempio:} Il dataset Zachary’s Karate Club (prevedere l'appartenenza a una fazione, Figura~\ref{fig:ZKCProblem});
        \item \textit{Parallelo:} Segmentazione semantica nelle immagini (etichettare ogni pixel); Part-of-Speech Tagging (assegnare una categoria grammaticale a ogni parola);
    \end{itemize}
    \item \textbf{Edge-level task:} Valutare le relazioni (link prediction) o le proprietà degli archi fra i singoli nodi;
    \begin{itemize}
        \item \textit{Esempio:} Prevedere se due persone si collegheranno in un social network (Link Prediction); determinare le relazioni spaziali tra oggetti in una scena. In questi compiti si costruisce spesso un grafo completamente connesso o un bipartite graph per esplorare le interazioni.
    \end{itemize}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/ZachsKarateClubProblem.png}
    \caption{Zachary’s Karate Club: a sinistra, la rete prima della divisione; a destra, i due gruppi dopo lo scisma.}
    \label{fig:ZKCProblem}
\end{figure}

\section{Computazione sui grafi}
La flessibilità dei grafi implica l’\textbf{assenza di una struttura fissa}. Grafi diversi (es. due molecole) hanno dimensioni variabili. Anche lo stesso grafo può essere rappresentato da matrici di adiacenza completamente diverse a seconda dell'ordine in cui si elencano i nodi. Questo rende la rappresentazione sensibile alle \textbf{permutazioni} dei nodi. La sfida principale è che il modello Deep Learning deve essere progettato per essere \textbf{invariante alla permutazione} (o \textbf{equi-variante}), ovvero il risultato finale del task (e.g la tossicità di una molecola) non deve cambiare se si riordinano i nodi.

\subsection{Ordine e sparsità}
Convertire un grafo in un vettore (o in una matrice di adiacenza) richiede un ordinamento dei nodi. Questo ordinamento è arbitrario e, come detto, porta a diverse rappresentazioni equivalenti. Nel contesto delle GNN, non si cerca di ordinare i nodi in modo canonico (cosa difficile e costosa per grafi grandi), ma si cerca di utilizzare tecniche di aggregazione (Message Passing) che sono \textit{intrinsecamente invarianti all'ordine}. La sparsità è una caratteristica strutturale (avere pochi archi rispetto al numero massimo possibile) che è cruciale per la scalabilità computazionale. Nei grandi grafi (e.g social network), $A$ è sparsa e ciò permette di evitare di memorizzare e computare interazioni inesistenti.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figure/OrderNode}
    \caption{Due rappresentazioni diverse di un alfabeto attraverso un grafo. Sebbene l’ordine dei nodi sia differente, il significato sottostante è identico.}
    \label{fig:ordNode}
\end{figure}

\subsection{La lista di adiacenza}
I grafi possono essere rappresentati in diverse modalità. La \textbf{Lista di Adiacenza} (Figura~\ref{fig:GraphComp}), è una delle più efficienti, specialmente per grafi sparsi. La connettività viene descritta tramite delle tuple (coppie di nodi e attributi dell'arco). Questa rappresentazione è preferita in termini di memoria rispetto alla Matrice di Adiacenza quando il grafo è molto sparso, poiché si memorizzano solo le connessioni esistenti. È importante notare che, sebbene l'esempio della figura usi valori scalari, nella pratica i modelli GNN operano con \textbf{vettori di caratteristiche} (feature vectors) (embedding) per nodi, archi e contesto globale.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/ComputGraph.png}
    \caption{Rappresentazione di un grafo mediante lista di adiacenza. Ogni nodo è collegato ad altri nodi secondo le coppie definite nella Adjacency List. Gli archi rappresentano le connessioni tra i nodi, mentre la lista di adiacenza descrive in forma strutturata quali nodi sono connessi tra loro.}
    \label{fig:GraphComp}
\end{figure}

\section{Graph Neural Network}
Le \textbf{Graph Neural Network} (GNN) modelli progettati per apprendere rappresentazioni (embedding) su strutture a grafo. Una GNN effettua una trasformazione che agisce su tutti gli attributi del grafo, mantenendo invariata la sua struttura topologica. Questi modelli adottano una filosofia \textbf{Graph-in}, \textbf{Graph-out}: prendono in input un grafo con feature associate a nodi, archi e contesto globale, e restituiscono un grafo trasformato, con la stessa struttura ma con rappresentazioni aggiornate. Queste architetture seguono il framework delle \textbf{Message Passing Neural Networks} (MPNN) introdotto da Gilmer et al. (2017)~\cite{gilmer2017neural}, e implementato secondo lo schema delle Graph Nets proposte da Battaglia et al. (2018)~\cite{battaglia2018relational}.

\subsection{La GNN più semplice}
La GNN più elementare ignora la connettività del grafo e sfrutta solo le \textit{feature} iniziali. Applica un \textbf{Multilayer Perceptron (MLP) separato} e \textbf{condiviso} a ciascun elemento:

\begin{itemize}
    \item Un MLP per ogni nodo ($v$), trasformando $x_v$ in $h_v$;
    \item Un MLP per ogni arco ($e$), trasformando $x_e$ in $h_e$;
    \item Un MLP per il vettore del contesto globale ($u$).
\end{itemize}

Questo \textit{layer} elementare apprende \textit{embedding} aggiornati per ogni componente, ma \textbf{non sfrutta} la topologia per scambiare informazioni.

\subsection{Pooling delle informazioni}

Nelle GNN, il termine \textit{pooling} (o \textbf{aggregazione invariante}) fa riferimento all’operazione di aggregazione delle informazioni provenienti da \textit{insiemi disordinati} di elementi del grafo, solitamente, l'insieme dei vicini di un nodo, con lo scopo di costruire una rappresentazione che non dipenda dall'ordine degli elementi. L'aggregrazione è cruciale nei task a livello di grafo. Il processo si articola infatti seguendo due passaggi fondamentali:
\begin{enumerate}
    \item \textbf{Raccolta:} Si raccolgono gli \textit{embedding} degli elementi correlati (e.g tutti i vicini $\mathcal{N}(v)$;
    \item \textbf{Aggregazione:} L'insieme raccolto viene aggregato tramite una funzione \textbf{invariante all'ordine} (tipicamente una somma, una media o il valore massimo), per ottenere un singolo embedding rappresentativo considerato un \textit{messaggio}.
\end{enumerate}

In un \textit{node-level task} ad esempio, se il nodo non ha feature iniziali, è necessario inferirle aggregando quelle dagli archi adiacenti (Figura~\ref{fig:aggEdg}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figure/AggEdges.png}
    \caption{In assenza di informazioni nei nodi, è possibile inferirle aggregando quelle presenti negli archi adiacenti.}
    \label{fig:aggEdg}
\end{figure}

\subsection{Message Passing}
Il \textbf{Message Passing} è la filosofia che integra la topologia del grafo nel processo di apprendimento, superando il limite dell'aggregazione non strutturata. Gli elementi del grafo scambiano informazioni con i loro vicini in modo iterativo, aggiornando i rispettivi embedding in base alla struttura del grafo. Il meccanismo si articola in tre fasi per aggiornare lo stato di un nodo $v$:


\begin{enumerate}
    \item \textbf{Messaggio (Raccolta):} Si raccolgono gli embedding dei nodi adiacenti $u\in\mathcal{N}(v)$, e se necessario, degli archi adiacenti $e_vu$;
    \item \textbf{Aggregazione:} I messaggi raccolti vengono aggregati in un unico vettore, tramite somma o media, garantendone l'invarianza all'ordine;
    \item \textbf{Aggiornamento:} Il risultato dell'aggregazione viene passato a una funzione di aggiornamento, solitamente un MLP o una funzione non lineare, per produrre un nuovo embedding $h_v^{(l+1)}$ per il nodo.
\end{enumerate}

\[
    h_v^{(l+1)}=\operatorname{UPDATE}^{(l)}\left(h_v^{(l)},\operatorname{AGGREGATE}^{(l)}(\{m_{vu}\forall u\in\mathcal{N}(v)\})\right)
\]
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/MessagePassing.png}
    \caption{Fasi della Message Passing: aggregazione dei nodi adiacenti, trasformazione tramite MLP, aggiornamento degli embedding.}
    \label{fig:messPass}
\end{figure}

Questo processo è locale: in uno strato ($l$), il nodo riceve informazioni solo dai vicini a distanza 1. Applicando $L$ strati di Message Passing, il nodo integra informazioni da tutti i vicini a distanza $L$ (il campo ricettivo del nodo).

\subsection{Rappresentazione globale}

Una delle limitazioni del Message Passing è la difficoltà di trasferire efficacemente informazioni tra nodi molto distanti, causando problemi di bottleneck o oversmoothing. Una soluzione è fornire a ciascun nodo una \textbf{Rappresentazione Globale} ($U$), nota anche come \textbf{Contesto Globale} o \textbf{Master Node}. Questo vettore $U$ è connesso a tutti gli archi e i nodi, agendo come un canale condiviso per lo scambio di informazioni globali tra le componenti del grafo.

\subsubsection{Conditioning}

Il \textbf{Conditioning} è un metodo efficace per arricchire l'embedding di un nodo integrando le informazioni locali con il contesto globale. L'idea si basa sull'aggiornamento di un nodo che non dipenda soltanto dai suoi vicini, ma anche dal contesto globale $U$:

\[
h_v^{(l+1)}=\operatorname{MLP}\left(h_v^{(l)}\|\operatorname{AGGREGATE}(\mathcal{N}(v))\|u^{(l)}\right)
\]

Questa tecnica aiuta a costruire rappresentazioni più informative e robuste, specialmente nei task a livello di grafo (Figura~\ref{fig:condit}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/Conditioning.png}
    \caption{Rappresentazione del Conditioning applicato all'embedding di un nodo, arricchito mediante gli embedding dei nodi e degli archi adiacenti, e quello relativo al contesto globale.}
    \label{fig:condit}
\end{figure}

\subsection{Filtri polinomiali sui grafi}
I \textbf{Filtri Polinomiali} sono lo strumento matematico chiave per estendere il concetto di convoluzione dal dominio regolare delle immagini al dominio irregolare dei grafi (approccio spettrale). Nei grafi, non si può usare un filtro fisso. Si usa il \textbf{Laplaciano del grafo ($L$)}, un operatore che funge da analogo del Gradiente/Laplaciano continuo e descrive la connettività.

\subsubsection{Il Laplaciano del grafo}

Dato un grafo \( G = (V, E) \) con matrice di adiacenza \( A \), dove \(A_{ij}\) ha valore unitario, qualora i nodi i e j sono connessi, la \textbf{Matrice diagonale dei Gradi} \( D \) è definita come:
\[
  D_{ii} = \sum_{j} A_{ij}
\]
Il \textbf{Laplaciano non normalizzato} del grafo si ricava tramite:
\[
  L = D - A
\]

Questo operatore misura "quanto un nodo differisce dalla media dei suoi vicini", una versione alternativa è il \textbf{Laplaciano normalizzato} utilizzato per fattori di stabilità:
\[
  \mathbf{L} = I - D^{-1/2} A D^{-1/2}
\]
Questo serve a rendere il contributo di ciascun nodo indipendente dal suo grado (cioè dal numero di connessioni). Tale operatore discreto è l’analogo del Laplaciano continuo impiegato in analisi matematica e fisica. Definito \( L \), è possibile costruire polinomi in \( L \):
\[
  p_w(L) = \sum_{k=0}^d w_k L^k
\]
dove:
\begin{itemize}
  \item \( w_k \) sono coefficienti, i parametri appresi dal modello;
  \item \( d \) è il grado del polinomio, che controlla la profondità del campo ricettivo.
\end{itemize}
Applicare l’operazione \( p_w(L) x \) consente di propagare le feature \( x \in \mathbb{R}^n \) tra nodi del grafo.

\subsubsection{Interpretazione: Convoluzione Locale}
Nel dominio dei grafi $L^k$ ha l'effetto di collegare ogni nodo con i suoi vicini a distranza $k$. La convoluzione con un filtro polinomiale agisce come un kernel con raggio $d$:
\begin{itemize}
  \item \( L^0 x = x \): il nodo vede solo se stesso;
  \item \( Lx \): il nodo riceve informazioni solo dai suoi vicini diretti;
  \item \( L^2 x \): il nodo integra le informazioni dai nodi entro $k$-hop.
\end{itemize}

\subsubsection{Esempio: Filtro di grado 1}
Il caso più semplice è proprio il Filtro di grado 1:
\[
  p_w(L) = w_0 I + w_1 L
\]
Applicandolo a un vettore \( x \), otteniamo:
\[
  x' = w_0 x + w_1 Lx
\]
Dove il primo termine ($w_0x$), manterrà le feature originali del nodo, mentre il secondo termine ($w_1Lx$) aggiunge le informazioni dai nodi adiacenti, il Laplaciano agisce come un operatore di differenziazione/smoothing. Questo filtro di grado 1 è la base concettuale della \textbf{Graph Convolutional Network} (GCN)~\cite{kipf2017semi}, dove ogni nodo aggiorna la propria rappresentazione combinando sé stesso e i suoi vicini.

\subsection{Chebyshev Polynomials (ChebNet)}

Nei filtri polinomiali, il calcolo diretto di $L^k$ è computazionalmente costoso e instabile (a causa degli autovalori di $L$ che possono essere grandi). Per risolvere questo problema, Michaël Defferrard et al.~\cite{defferrard2016convolutional} introducono i \textbf{Polinomi di Chebyshev} come base ortogonale per approssimare il filtro:

\[
x'=\sum_{k=0}^K w_kT_k(\tilde{L})x
\]

\begin{itemize}
    \item $w_k$: Parametri appresi dal filtro (uno per ogni ordine del polinomio);
    \item $T_k(\cdot) \rightarrow$ Rappresenta la propagazione dell’informazione a distanza $k$, ed è il $k$-esimo Polinomio di Chebyshev;
    \item $\tilde{L} = \frac{2L}{\lambda_{\text{max}}(L)} - I$: È il Laplaciano scalato e traslato per avere autovalori nell'intervallo $[-1,1]$, garantendo stabilità.
\end{itemize}

I Polinomi di Chebyshev hanno una forma ricorsiva efficiente, permettendo di calcolare $T_k(\tilde{L})x$ senza calcolare esplicitamente gli autovalori, principale causa di instabilità che portava a un costo computazionale pari a $O(n^3)$.
\[
\begin{aligned}
  T_0(x) &= 1 \\
  T_1(x) &= x \\
  T_{k+1}(x) &= 2x T_k(x) - T_{k-1}(x)
\end{aligned}
\]

\section{Modern Graph Neural Networks}
Le GNN moderne si sono evolute dal framework spettrale (ChebNet, GCN di primo grado) verso l'approccio \textbf{spaziale} (Message Passing) che utilizza tecniche di aggregazione più raffinate, mantenendo l'invarianza all'ordine.

\subsection{Graph Convolutional Network (GCN)}
La \textbf{Graph Convolutional Network (GCN)} è la rete che ha reso l'approccio GNN pratico. Essa non utilizza i polinomi di Chebyshev per esteso, ma è una semplificazione del filtro di grado 1 (la GCN è un filtro polinomiale limitato a $K=1$). La formulazione operativa (usando la matrice di adiacenza rinormalizzata $\tilde{A}$) è:

\[
    H^{(l+1)}=\sigma(\tilde{A}H^{(l)}W^{(l)})
\]

La sua forma di \textit{Message Passing} è la seguente:

\[
    h_v^{(k)}=\sigma\left(W^{(k)}\cdot\sum_{u\in\mathcal{N}(v)\cup\{v\}}\frac{1}{\sqrt{\tilde{D}_{vv}\tilde{D}_{uu}}}h_u^{(k-1)}\right)
\]

\begin{itemize}
    \item La \textbf{somma pesata} (normalizzata dal grado $\tilde{D}$) sulle feature dei vicini è la fase di \textbf{aggregazione};
    \item L'operatore \textbf{lineare} ($W$) seguito dalla non linearità ($\sigma$) è la fase di \textbf{aggiornamento};
    \item Le matrici $W^{(k)}$ sono condivise tra i nodi, garantendo la scalabilità.
\end{itemize}

\subsection{Graph Attention Network (GAT)}
La \textbf{Graph Attention Network (GAT)} si distacca dalla normalizzazione statica della GCN e introduce un \textbf{meccanismo di attenzione} che assegna pesi dinamici all'aggregazione:

\[
    h_v^{(k)} = f^{(k)}\left(W^{(k)} \cdot \sum_{u \in \mathcal{N}(v)\,\cup\, \{v\}} \alpha_{vu}^{(k-1)} h_u^{(k-1)}\right)
\]

\begin{itemize}
    \item $\alpha_{vu}^{(k-1)}$: Coefficiente di attenzione appreso;
    \item \textbf{Calcolo dell'attenzione:} Il punteggio di attenzione $\alpha_{vu}$ è calcolato tramite una funzione di compatibilità (e.g una single-layer feedforward network) tra i nodi $v$ e $u$ e normalizzato con \textbf{softmax} sui vicini:
    \[
        \alpha_{vu}^{(k)} = \frac{\operatorname{exp}(e_{vu}^{(k)})}{\sum_{w\in\mathcal{N}(v)}\operatorname{exp}(e_{vw}^{(k)})}
    \]
\end{itemize}
Questo permette alla GAT di dare maggiore importanza ai vicini più rilevanti per il task, aumentando l'espressività rispetto alla GCN.

\subsection{Graph SAGE}

\textbf{Graph SAGE} (Graph Sample and AggregatE) è un'architettura progettata per l'apprendimento induttivo e la scalabilità su grafi di grandi dimensioni. Invece di apprendere un embedding fisso per ogni nodo, GraphSAGE apprende una funzione di aggregazione generale che può essere applicata anche a nodi non visti durante il training (apprendimento induttivo). Il principio chiave è il campionamento locale: ogni nodo aggiorna la sua rappresentazione aggregando le feature di un sottoinsieme campionato dei suoi vicini:

\[
    h_v^{(k)} = \sigma\left(W^{(k)} \cdot \text{AGGREGATE}^{(k)}\left(\{h_v^{(k-1)}\} \cup \{\operatorname{SAMPLE(}h_u^{(k-1)}, \forall u \in \mathcal{N}(v)\}\right)\right)
\]
Dove \( \text{AGGREGATE}^{(k)} \) può essere una media, una somma o una rete neurale più complessa (e.g LSTM o max-pooling). L’approccio di Graph SAGE è particolarmente utile per scenari di \textit{streaming} o \textit{large-scale learning}, poiché permette di scalare a grafi di grandi dimensioni grazie al campionamento locale e al riutilizzo della stessa funzione aggregatrice.

\subsection{GIN}
La \textbf{Graph Isomorphism Network (GIN)} è stata proposta con l’obiettivo di massimizzare il potere discriminante delle GNN, equiparandolo a quello del Weisfeiler-Lehman Test (WL Test). GIN dimostra che le GNN più potenti non hanno bisogno di meccanismi complessi come l'attenzione o la normalizzazione statica, ma di una semplice ma espressiva funzione di aggregazione e aggiornamento:

\[
    h_v^{(k)} = \text{MLP}^{(k)} \left((1 + \epsilon^{(k)}) \cdot h_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k-1)} \right)
\]
Dove:
\begin{itemize}
    \item \( \epsilon^{(k)} \): è un parametro che bilancia il contributo del nodo rispetto ai vicini;
    \item \( \text{MLP}^{(k)} \): Applica una trasformazione non lineare sulla somma, fungendo da hashing delle etichette del WL Test;
    \item \(\sum_{u\in\mathcal{N}(v)} h_u^{(k-1)} \): Semplice somma dei vicini (un'operazione invariante all'ordine e massimamente espressiva per il WL Test).
\end{itemize}
GIN ottiene la massima capacità di distinguere strutture complesse pur mantenendo una formula di aggregazione apparentemente semplice.

\begin{sidewaystable}[htbp]
    \centering
    \caption{Confronto tra le principali architetture di Graph Neural Networks.}
    \scriptsize
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Caratteristica} & \textbf{GCN} & \textbf{GAT} & \textbf{GraphSAGE} & \textbf{GIN} \\
    \hline
    \textbf{Tipo Aggregazione} & Media normalizzata & Attenzione pesata & Custom (media, LSTM, pooling) & Somma + MLP \\
    \hline
    \textbf{Pesatura dei vicini} & Statica (matrice normalizzata) & Dinamica (self-attention) & Parametrica o fissa & Fissa (somma) \\
    \hline
    \textbf{Apprendimento Induttivo} & No & Parziale & Sì & No \\
    \hline
    \textbf{Espressività} & Limitata & Superiore a GCN & Dipende dall’aggregatore & Massima (WL test) \\
    \hline
    \textbf{Funzione di aggiornamento} & Lineare + ReLU & Lineare + Attenzione & MLP + Aggregazione & MLP su somma \\
    \hline
    \textbf{Parametri appresi} & W & W, Attenzione & W, Aggregatore (opz.) & MLP, $\epsilon$ \\
    \hline
    \textbf{Sensibile alla struttura del grafo} & Sì & Sì & Sì & Sì (con maggiore discriminatività) \\
    \hline
    \textbf{Scalabilità} & Buona & Limitata (per grafi molto grandi) & Alta (con campionamento) & Media \\
    \hline
    \textbf{Proprietà chiave} & Semplicità & Flessibilità & Generalizzazione & Massima capacità discriminante \\
    \hline
    \end{tabular}
\label{tab:GNN_comparison}
\end{sidewaystable}

