\chapter{Multi Head Attention}
Questo capitolo esplora le tecniche avanzate di attenzione, in particolare quelle Multi-Head. Analizzando le ottimizzazioni che hanno reso possibile l'implementazione efficiente dei modelli di linguaggio di grandi dimensioni. Tratteremo le tecniche di Caching KV, le varianti Multi Query Attention (MQA) e Grouped Query Attention (GQA), gli Embeddings Posizionali Rotazionali (RoPE) e la Multi Head Latent Attention (MLA).


\section{Introduzione}

L'architettura Transformer ha rivoluzionato il campo del Deep Learning nel campo del Natural Language Processing (NLP). Tuttavia, con l'aumento delle dimensioni dei modelli e delle sequenze di input, l'efficienza computazionale e di memoria del meccanismo di attenzione Multi-Head è diventata una sfida critica, esploriamo insieme le tecniche avanzate volte a ottimizzare l'attenzione, mantenendo o migliorando le prestazioni dei modelli.

\section{KV Cache}

Durante la fase d'inferenza, nei modelli autoregressivi, il calcolo dell'attenzione per ogni nuovo token richiede l'accesso alle rappresentazioni di tutti i token precedenti. Senza ottimizzazioni, questo comporterebbe il ricalcolo completo delle matrici Key (K) e Value (V) per ogni posizione ad ogni step di generazione. La tecnica del \textbf{KV Cache} risolve questo problema memorizzando le matrici Key e Value calcolate nei passi precedenti all'interno della Cache:

\begin{equation}
    \text{Attention}(Q_t, K_{1:t}, V_{1:t}) = \text{softmax}\left(\frac{Q_t K_{1:t}^T}{\sqrt{d_k}}\right) V_{1:t}
\end{equation}

dove $Q_t$ è la query per il token corrente alla posizione $t$, mentre $K_{1:t}$ e $V_{1:t}$ rappresentano tutte le chiavi e valori a partire dalla prima posizione, fino a giungere alla posizione $t$.

\subsection{Implementazione della Cache}
Ad ogni step di generazione:
\begin{enumerate}
    \item Si calcola la nuova coppia key-value per il token corrente;
    \item Si concatena questa coppia alla Cache esistente;
    \item Si utilizza la Cache aggiornata per calcolare l'attenzione.
\end{enumerate}

Questo approccio riduce la complessità temporale da $o(n^2)$ a $o(n)$ per ogni nuovo token, dove $n$ è la lunghezza della sequenza.

\subsection{Requisiti di Memoria}
La dimensione del KV Cache per un layer di attenzione Multi-Head standard è:

\[
    \text{Cache Size} = 2 \times \text{batch\_size} \times \text{num\_heads} \times \text{seq\_length} \times \text{head\_dim}
\]

Il fattore 2 deriva dalla necessità di memorizzare sia la matrice Key che la matrice Value. Questa crescita lineare con la lunghezza della sequenza rappresenta una limitazione significativa per sequenze molto lunghe.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figure/KVCache}
    \caption{Illustrazione del meccanismo KV Cache: i vettori query correnti vengono moltiplicati con i vettori key memorizzati nel cache per calcolare gli attention scores, evitando il ricalcolo delle rappresentazioni dei token precedenti.}
    \label{fig:KVC}
\end{figure}
\section{Multi Query Attention (MQA)}

La \textbf{MQA} (Multi Query Attention), introdotta nei modelli PaLM (Google) e Falcon, affronta il problema della memoria del KV Cache attraverso una strategia di condivisione delle rappresentazioni. L'idea è quella di utilizzare un'unica coppia Key-Value condivisa tra tutte le teste di attenzione, mantenendo invece Query separate per ogni testa:

\begin{align*}
    Q_i &= x W_i^Q \quad \text{per } i = 1, \ldots, H \\
    K &= x W^K \\
    V &= x W^V
\end{align*}

dove $H$ è il numero di teste di attenzione, $x$ è la rappresentazione di input e $W_i^Q$, $W^K$, $W^V$ sono le matrici di proiezione.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figure/mha_vs_mqa_comparison.png}
    \caption{Architettura di Multi-Head Attention (MHA) vs Multi-Query Attention (MQA). MQA utilizza Key e Value condivisi tra tutte le teste di attenzione, mantenendo Query separate, riducendo così i requisiti di memoria del cache.}
    \label{fig:mha_vs_mqa}
\end{figure}

\subsection{Calcolo dell'Attenzione}
L'attenzione per ogni testa viene calcolata come:

\[
    \text{head}_i = \text{Attention}(Q_i, K, V) = \text{softmax}\left(\frac{Q_i K^T}{\sqrt{d_k}}\right) V
\]

L'output finale è ottenuto concatenando tutte le teste:

\[
    \text{MQA}(h) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W^O
\]

\subsection{Riduzione della Memoria}
Con MQA, la dimensione del KV Cache diventa:

\[
    \text{MQA Cache Size} = 2 \times \text{batch\_size} \times 1 \times \text{seq\_length} \times \text{head\_dim}
\]

Questo rappresenta una riduzione di un fattore $H$ (numero di teste) rispetto all'attenzione Multi-Head standard, portando a significativi risparmi di memoria per modelli con molte teste di attenzione.

\subsection{Prestazioni}
Gli esperimenti mostrano che MQA mantiene prestazioni competitive rispetto all'attenzione Multi-Head classica, pur riducendo drasticamente i requisiti di memoria. Questo trade-off favorevole ha reso MQA una scelta popolare per modelli di grandi dimensioni.

\section{Grouped Query Attention (GQA)}

La \textbf{GQA} (Grouped Query Attention) invece, utilizzata in modelli come LLaMA 2~\cite{touvron2023llama}, LLaMA 3~\cite{touvron2024llama3} e Mistral 7B~\cite{mistral2023mistral7b}, rappresenta un compromesso tra Multi Head Attention classica e Multi Query Attention. Invece di utilizzare una singola coppia Key-Value per tutte le teste (MQA) o coppie separate per ogni testa (MHA), GQA organizza le teste in gruppi che condividono le stesse rappresentazioni Key-Value. Dato un numero totale di teste $H$ e un numero di gruppi $G$, ogni gruppo contiene $\frac{H}{G}$ teste di query che condividono la stessa coppia Key-Value:

\begin{align*}
    Q_{g,i} &= x W_{g,i}^Q \quad \text{per gruppo } g, \text{testa } i \\
    K_g &= x W_g^K \quad \text{per gruppo } g \\
    V_g &= x W_g^V \quad \text{per gruppo } g
\end{align*}

\subsection{Flessibilità della Configurazione}
GQA offre flessibilità nella scelta del numero di gruppi:
\begin{itemize}
    \item $G = H$: equivale a Multi Head Attention standard;
    \item $G = 1$: equivale a Multi Query Attention;
    \item $1 < G < H$: compromesso ottimale tra prestazioni e efficienza.
\end{itemize}

\subsection{Dimensione del Cache}
La dimensione del KV Cache per GQA è:

\[
\text{GQA Cache Size} = 2 \times \text{batch\_size} \times G \times \text{seq\_length} \times \text{head\_dim}
\]

dove $G$ è il numero di gruppi, fornendo una riduzione di memoria di un fattore $\frac{H}{G}$.
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figure/GPA.png}
    \caption{Architettura di un meccanismo di attenzione multi-head con quattro teste (Head 1-4). Ogni testa elabora un gruppo di Query Vectors in parallelo con Key Vectors, producendo matrici di attenzione indipendenti che possono essere successivamente combinate.}
    \label{fig:gpa}
\end{figure}
\subsection{Prestazioni}
GQA dimostra prestazioni superiori rispetto a MQA mantenendo significativi vantaggi in termini di memoria rispetto a MHA standard. Questa caratteristica l'ha resa una scelta popolare per molti modelli moderni di grandi dimensioni.

\section{Limiti degli Embeddings}
Gli embeddings posizionali assoluti presentano diverse limitazioni, due degli aspetti princiapali per cui si definiscono abbastanza limitati sono i seguenti:

\begin{itemize}
    \item \textbf{Lunghezza di Sequenza Limitata:} I modelli possono rappresentare posizioni solo fino a un limite predefinito durante l'addestramento, non potendo rappresentare posizioni oltre quel limite;
    \item \textbf{Indipendenza delle Posizioni:} Ogni embedding posizionale è indipendente dagli altri, questo significa che per il modello, la differenza fra la posizione 1 e la posizione 2 è la stessa che vi è fra la posizione 2 e la posizione 500, impedendo al modello di comprendere le relazioni relative tra posizioni.
\end{itemize}

La mancanza di posizionamento relativo può ostacolare la capacità del modello sul comprendere la struttura linguistica, dove le relazioni tra parole dipendono, spesso dalla loro distanza relativa. Gli embeddings posizionali relativi, come quelli utilizzati nel modello T5, si concentrano sulle distanze tra coppie di token e non sulle posizioni assolute. Tuttavia, presentano sfide pratiche:

\begin{itemize}
    \item \textbf{Problemi di Prestazioni:} Possono essere più lenti, specialmente per sequenze lunghe;
    \item \textbf{Complessità nel KV Cache:} Ogni token aggiuntivo altera l'embedding per tutti gli altri token, complicando l'uso efficiente della cache.
\end{itemize}

Proprio a causa di queste complessità ingegneristiche, gli Embedding Posizionali non sono stati adottati largamente, soprattutto nel contesto dei \textit{Large Language Models} (LLM), differentemente dall'utilizzo di quelli rotazionali che vedremo a breve, i quali si basano sulle \textit{Matrici di Rotazione}.

\subsection{Matrici di Rotazione}
Le \textbf{Matrici di Rotazione} costituiscono la base matematica dei \textbf{RoPE} (Rotational Positional Embeddings). Una matrice di rotazione 2D è definita come:

\begin{equation}
    R(\theta) = \begin{pmatrix}
    \cos\theta & -\sin\theta \\
    \sin\theta & \cos\theta
    \end{pmatrix}
\end{equation}

Quando questa matrice viene applicata a un vettore, tramite un prodotto, questa matrice cambia l'angolo del vettore mantenendone invariata la sua lunghezza.

\subsection{Formulazione di RoPE}
Proprio su questo principio si basano i Rotary Positional Embedding, differendo dagli Embedding Posizionali Assoluti e Relativi, ch assegnano uno a ogni posizione un vettore che verrà sommato alla parola, non addattandosi a frasi lunghe, l'altro sulla distanza fra parole, complicando però il calcolo dell'attenzione. \textbf{RoPE} propone un'idea molto elegante, ispirata alla geometria. Pensiamo a un orologio e immaginiamo che ogni parola sia rappresentata da una lancetta (un vettore) che parte dal centro del quadrante, la prima parola verrà ruotata di dieci gradi, la terza di trenta gradi e così via. Questa è considerata un'idea geniale poiché quando il modello calcola l'attenzione tra due parole (usando il prodotto scalare), il risultato dipende solo dalla differenza angolare tra le due lancette. Quindi, applicando una rotazione basata sulla posizione assoluta, il modello impara automaticamente le relazioni di posizione relativa "gratis". RoPE per esempio applica rotazioni position-dependent alle rappresentazioni Query e Key, per due token alle posizioni $m$ e $n$:

\begin{align*}
    q_m &= R(\theta, m) \cdot W^Q h_m \\
    k_n &= R(\theta, n) \cdot W^K h_n
\end{align*}

dove $R(\theta, \operatorname{pos})$ è la matrice di rotazione dipendente dalla posizione, in questo caso l'ennesima e l'emmesima posizione. Le matrici di rotazione possiedono la proprietà fondamentale:

\[
    R(\theta, m)^T R(\theta, n) = R(\theta, n-m)
\]

Questa proprietà, consente al prodotto scalare tra query e key di dipendere solo dalla loro distanza relativa $(n-m)$, ottenendo così un posizionamento relativo naturale. Torna utile nel momento in cui si vuole calcolare l'attention score come quanto segue, il quale grazie a questa proprietà subisce una trasformazione nel suo calcolo evidenziandone come lo score dipenda solo dalla differenza:

\[
    \operatorname{score}= q_m\cdot k_n = q^TR_m^TR_nk = q^TR_{m-n}k
\]

Per concludere analizziamo i vantaggi che offre RoPE, i quali sono diversi rispetto agli embedding tradizionali che abbiamo sempre considerato:
\begin{itemize}
    \item \textbf{Posizionamento Relativo:} Intrinsecamente codifica le relazioni relative, senza effettuare dei calcoli complessi;
    \item \textbf{Lunghezza Flessibile:} Può gestire sequenze più lunghe di quelle viste durante l'addestramento, perché essendo una rotazione, può girare all'infinito;
    \item \textbf{Efficienza:} È una tecnica che non aggiunge parametri da apprendere e si integra facilmente con le tecniche di caching KV;
    \item \textbf{Adozione Diffusa:} Utilizzato in modelli come RoFormer~\cite{su2021roformer}, LLaMA 2~\cite{touvron2023llama}, LLaMA 3~\cite{touvron2024llama3}, Gemma~\cite{google2024gemma}, PaLM~\cite{chowdhery2022palm}, GPT-NeoX~\cite{black2022gptneox}, Falcon~\cite{penedo2023falcon}, DeepSeek V2~\cite{deepseek2023v2}, DeepSeek V3~\cite{deepseek2024v3}, DeepSeek R1~\cite{deepseek2024r1}.
\end{itemize}

\section{Multi Head Latent Attention (MLA)}

La \textbf{MLA} (Multi Head Latent Attention), è stata introdotta nei modelli DeepSeek V2, V3 e R1, rappresentando un approccio più avanzato per ridurre i requisiti di memoria del KV Cache. L'idea centrale è comprimere l'input dell'attenzione in uno spazio latente a bassa dimensionalità come visibile nella parte più a destra della Figura~\ref{fig:diffAttention}.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/DifferentAttention.png}
    \caption{Rappresentazioni dei diversi meccanismi dell'attenzione presentati in questo capitolo da sinistra verso destra: Multi Head Attention, Grouped Query Attention, Multi Query Attention e Multi-Head Latent Attention.}
    \label{fig:diffAttention}
\end{figure}

\subsection{Principio Fondamentale}
MLA comprime l'input dell'attenzione $h_t$ in un vettore latente a bassa dimensionalità $c_t^{KV}$ con dimensione $d_c$, dove $d_c \ll h_n \cdot d_h$:

\[
    c_t^{KV} = W^{DKV}h_t
\]

dove $W^{DKV}$ è la matrice di compressione che riduce la dimensionalità da $(h_n \cdot d_h)$ a $d_c$.

\subsection{Recupero delle Chiavi e Valori}
Quando necessario per il calcolo dell'attenzione, il vettore latente viene mappato nuovamente nello spazio ad alta dimensionalità:

\begin{align*}
    K_t &= c_t^{KV} W^{UK} \\
    V_t &= c_t^{KV} W^{UV}
\end{align*}

dove $W^{UK}$ e $W^{UV}$ sono matrici di up-projection che mappano dallo spazio latente allo spazio originale.

\subsection{Trattamento delle Query}
Analogamente, le query possono essere trattate attraverso lo spazio latente:

\begin{align*}
    c_t^Q &= h_t W^{DQ} \\
    Q_t &= c_t^Q W^{UQ}
\end{align*}

\subsection{RoPE Disaccoppiato}
L'integrazione di RoPE con MLA presenta sfide tecniche. Nel caso standard, $W^{UK}$ può essere "assorbito" in $W^Q$, riducendo ulteriormente l'uso di memoria. Tuttavia, con RoPE, la matrice di rotazione position-dependent si interpone tra $(W^Q)^T$  e $W^{UK}$, impedendo questo assorbimento. Per risolvere questa problematica, MLA introduce il così chiamato "RoPE disaccoppiato":
\begin{itemize}
    \item Vettori query aggiuntivi vengono introdotti insieme a un vettore key condiviso;
    \item Questi vettori aggiuntivi sono utilizzati esclusivamente per il processo RoPE;
    \item Le chiavi originali rimangono isolate dalla matrice di rotazione.
\end{itemize}

\subsection{Calcolo dell'Attenzione MLA}
Il processo completo di calcolo dell'attenzione MLA include:

\begin{enumerate}
    \item Compressione dell'input nello spazio latente;
    \item Up-projection per recuperare keys e values;
    \item Applicazione del RoPE disaccoppiato;
    \item Calcolo dell'attenzione standard;
    \item Aggregazione dei risultati.
\end{enumerate}

\subsection{Riduzione della Memoria}
La dimensione del cache per MLA è drasticamente ridotta:
\[
    \text{MLA Cache Size} = \text{batch\_size} \times \text{seq\_length} \times d_c
\]
dove $d_c$ è significativamente più piccolo della dimensione originale, risultando una grande riduzione di memoria di diversi ordini di grandezza.

\subsection{Prestazioni}
Gli esperimenti mostrano che MLA mantiene prestazioni competitive o superiori rispetto alle tecniche precedenti, pur ottenendo le maggiori riduzioni di memoria tra tutte le tecniche discusse. Questo ha reso MLA particolarmente attraente per modelli che devono gestire sequenze molto lunghe o operare con risorse computazionali limitate.

\section{Confronto delle Tecniche}

\subsection{Analisi Comparativa}
La tabella seguente riassume le caratteristiche principali delle diverse tecniche di attenzione:

\begin{table}[htbp]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Tecnica} & \textbf{Cache Size} & \textbf{Qualità} & \textbf{Complessità} & \textbf{Adozione} \\
    \hline
    Multi Head Attention & $2 \times B \times H \times L \times D$ & Alta & Bassa & Standard \\
    Multi Query Attention & $2 \times B \times 1 \times L \times D$ & Media & Bassa & Diffusa \\
    Grouped Query Attention & $2 \times B \times G \times L \times D$ & Alta & Bassa & Molto diffusa \\
    Multi Head Latent Attention & $B \times L \times d_c$ & Alta & Alta & Emergente \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Confronto delle tecniche di attenzione avanzate}
    \label{tab:attention_comparison}
\end{table}

dove $B$ è la batch size, $H$ il numero di teste, $L$ la lunghezza della sequenza, $D$ la dimensione delle teste, $G$ il numero di gruppi, e $d_c$ la dimensione compressa.

\subsection{Scelta della Tecnica Ottimale}
La scelta della tecnica di attenzione dipende da diversi fattori:

\begin{itemize}
    \item \textbf{Risorse di Memoria:} MLA per limitazioni estreme, GQA per compromessi bilanciati;
    \item \textbf{Lunghezza delle Sequenze:} RoPE per sequenze lunghe, MLA per sequenze molto lunghe;
    \item \textbf{Prestazioni:} GQA per il miglior equilibrio prestazioni-efficienza;
    \item \textbf{Semplicità di Implementazione:} MQA per implementazioni rapide.
\end{itemize}

\section{Implementazioni Pratiche}

L'implementazione efficiente di queste tecniche richiede considerazioni specifiche:

\begin{itemize}
    \item \textbf{Gestione della Memoria:} Allocazione dinamica del Cache per sequenze di lunghezza variabile;
    \item \textbf{Parallelizzazione:} Ottimizzazione per GPU e TPU;
    \item \textbf{Precision:} Uso di mixed precision per ridurre ulteriormente l'uso di memoria;
    \item \textbf{Batch Processing:} Gestione efficiente di batch con sequenze di lunghezza diversa.
\end{itemize}

\subsection{Trade-off Prestazioni-Memoria}
Ogni tecnica presenta specifici trade-off:

\begin{description}
    \item[MQA] Massima semplicità implementativa con buone riduzioni di memoria;
    \item[GQA] Miglior equilibrio tra prestazioni e efficienza;
    \item[MLA] Massima riduzione di memoria con complessità implementativa maggiore;
    \item[RoPE] Miglior gestione delle sequenze lunghe con overhead computazionale minimo.
\end{description}

\section{Tendenze Future e Sviluppi}

Le direzioni future includono la così detta \textbf{Sparse Attention}, volta a ridurre la complessità quadratica dell'attenzione, l'\textbf{Adaptive Attention}, introducendo meccanismi che adattano la complessità alla difficoltà del task, avanzate tecniche di compressione più sofisticate per MLA, e ovviamente delle ottimizzazioni specifiche per le architetture hardware emergenti. Una cosa da considerare è come queste ottimizzazioni delle tecniche abbiano permesso a modelli di diventare enormi quasi 1T di parametri, garantire un inferenza efficiente sull'hardware dei consumatori, dare la possibilità di processare dei documenti molto lunghi e la possibilità infine di effettuare il deployment su dispositivi mobili, tutte non cose da poco. Le tecniche avanzate di attenzione Multi-Head rappresentano progressi fondamentali nell'ottimizzazione dei modelli Transformer. Dall'introduzione del KV Cache per accelerare l'inferenza, attraverso le varianti MQA e GQA per ridurre l'uso di memoria, fino agli Embeddings Posizionali Rotatori per una migliore gestione delle sequenze lunghe e alla rivoluzionaria MLA per compressioni estreme. Ognuna di queste innovazioni ha contribuito a rendere i modelli di linguaggio di grandi dimensioni più pratici e accessibili. La scelta della tecnica ottimale dipende dai requisiti specifici dell'applicazione, dalle risorse disponibili e dai trade-off accettabili tra prestazioni ed efficienza. Con il continuo sviluppo di nuove architetture e l'aumento delle dimensioni dei modelli, queste ottimizzazioni continueranno a evolversi, aprendo nuove possibilità per applicazioni sempre più sofisticate del Deep Learning nel processamento del linguaggio naturale. L'adozione diffusa di queste tecniche nei modelli moderni testimonia la loro efficacia e importanza pratica. Comprendere questi meccanismi è essenziale per chiunque lavori con modelli Transformer avanzati e rappresenta la base per future innovazioni nel campo.